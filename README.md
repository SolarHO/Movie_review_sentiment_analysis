---

# Movie_review_sentiment_analysis

## ğŸ§  Transformer ê¸°ë°˜ í•œêµ­ì–´ ê°ì„± ë¶„ì„ í”„ë¡œì íŠ¸

---

**í•œêµ­ì–´ ì˜í™” ë¦¬ë·°(NSMC)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ KoBERT / KoELECTRA ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ ê°ì„±ì„ ë¶„ë¥˜í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.**
<br>(ì´ì§„ ë¶„ë¥˜: ê¸ì • / ë¶€ì •)

---

ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

**í”„ë¡œì íŠ¸ëª…:** Movie Review Sentiment Analysis
<br>**ì‚¬ìš© ëª¨ë¸:**
<br>  - ğŸŸ¦ KoBERT (SKTBrain)
<br>  - ğŸŸ© KoELECTRA (monologg)
<br>**ë°ì´í„°ì…‹:** NSMC (Naver Sentiment Movie Corpus)
<br>**ë¬¸ì œ ìœ í˜•:** ë¬¸ì¥ ë‹¨ìœ„ ê°ì„± ë¶„ì„ (Binary Classification)
<br>**í”„ë ˆì„ì›Œí¬:** PyTorch, Hugging Face Transformers
<br>**ê°œë°œ í™˜ê²½:** Google Colab (ë¬´ë£Œ GPU + CPU)

---

**ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°**

```
Movie_review_sentiment_analysis/
  â”œâ”€ kobert_nsmc_exp2/              # Fine-tuned KoBERT ëª¨ë¸ (LFS)
  â”œâ”€ koelectra_nsmc_exp1/           # Fine-tuned KoELECTRA ëª¨ë¸ (LFS)
  â”œâ”€ nsmc_train_kobert.ipynb        # KoBERT í•™ìŠµ ë…¸íŠ¸ë¶
  â”œâ”€ nsmc_train_koelectra.ipynb     # KoELECTRA í•™ìŠµ ë…¸íŠ¸ë¶
  â”œâ”€ inference_cpu.py               # CPU í™˜ê²½ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
  â””â”€ README.md
```

---

**ğŸ“˜ ë°ì´í„°ì…‹(NSMC)**

- ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° 200,000ê±´
- Train: 150,000 / Test: 50,000
- ê¸ì •(1), ë¶€ì •(0) ë ˆì´ë¸” êµ¬ì„±
- ì „ì²˜ë¦¬: ê²°ì¸¡/ê³µë°± ì œê±°, 128 í† í°ìœ¼ë¡œ íŒ¨ë”©

---

**ğŸ§© ëª¨ë¸ ì„¤ëª…**

**ğŸ”µ KoBERT (SKTBrain)**
- í•œêµ­ì–´ í˜•íƒœì— ìµœì í™”ëœ BERT ëª¨ë¸
- í† í¬ë‚˜ì´ì €: SentencePiece
- ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ë¡œ ì‚¬ìš©

**ğŸŸ¢ KoELECTRA (monologg)**
- ELECTRA Discriminator ê¸°ë°˜
- BERT ëŒ€ë¹„ í›ˆë ¨ íš¨ìœ¨ ë†’ê³  ì„±ëŠ¥ ìš°ìˆ˜
- ì‹¤í—˜ ê²°ê³¼ ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ ê¸°ë¡

---

**âš™ï¸ í•™ìŠµ í™˜ê²½**

- Google Colab Free GPU(T4)
- í•™ìŠµ ì„¤ì •:

| í•­ëª©         | ê°’                             |
| ---------- | ----------------------------- |
| Max Length | 128                           |
| Batch Size | 32                            |
| Optimizer  | AdamW                         |
| LR         | 5e-5(KoBERT), 3e-5(KoELECTRA) |
| Epochs     | 3                             |
| Loss       | Cross Entropy                 |

---

**ğŸ“Š ì‹¤í—˜ ê²°ê³¼**

**KoBERT vs KoELECTRA ì„±ëŠ¥ ë¹„êµ**
| Exp | Backbone                                               | Epoch | LR   | Valid Acc  | Valid F1   |
| --- | ------------------------------------------------------ | ----- | ---- | ---------- | ---------- |
| 1   | KoBERT (`monologg/kobert`)                             | 3     | 5e-5 | **0.8862** | **0.8879** |
| 2   | KoELECTRA (`monologg/koelectra-base-v3-discriminator`) | 3     | 3e-5 | **0.9112** | **0.9125** |

ğŸ‘‰ **KoELECTRAê°€ ì•½ +2.5%p ë” ë†’ì€ ì •í™•ë„**ë¥¼ ë³´ì´ë©° ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ê¸°ë¡í•¨.<br>
ğŸ‘‰ Epoch 3ì—ì„œ KoELECTRAëŠ” ì•½ê°„ì˜ ê³¼ì í•© ì¡°ì§ì´ ìˆìœ¼ë‚˜, F1-scoreëŠ” ìµœê³ ì¹˜ë¥¼ ê¸°ë¡.

---

**ğŸ¤– ì¶”ë¡ (Inference) ì˜ˆì‹œ (CPU í™˜ê²½ ê°€ëŠ¥)**

**â–¶ï¸ ì˜ˆì‹œ ì½”ë“œ**
```
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

device = torch.device("cpu")

# KoELECTRA ëª¨ë¸ ë¡œë“œ
model_dir = "./koelectra_nsmc_exp1"
tokenizer = AutoTokenizer.from_pretrained(
    "monologg/koelectra-base-v3-discriminator",
    use_fast=True,
    trust_remote_code=True,
)
model = AutoModelForSequenceClassification.from_pretrained(
    model_dir,
    local_files_only=True,
)
model.to(device)
model.eval()

MAX_LEN = 128
id2label = {0: "ë¶€ì •", 1: "ê¸ì •"}

def predict_sentiment(text: str):
    inputs = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN,
        return_tensors="pt",
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=-1)
        pred = int(torch.argmax(probs))
        return id2label[pred], float(probs[0][pred])

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥
sentences = [
    "ì§„ì§œ ë„ˆë¬´ ì¬ë°ŒëŠ” ì˜í™”ì˜€ë‹¤. ë˜ ë³´ê³  ì‹¶ë‹¤.",
    "ìŠ¤í† ë¦¬ë„ ë³„ë¡œê³  ì—°ê¸°ë„ ì–´ìƒ‰í•´ì„œ ì‹œê°„ ì•„ê¹Œì› ë‹¤.",
    "ê·¸ëƒ¥ì €ëƒ¥ ë³¼ ë§Œí–ˆì§€ë§Œ ë‹¤ì‹œ ë³¼ ì •ë„ëŠ” ì•„ë‹ˆë‹¤.",
    "ëˆì´ ì „í˜€ ì•„ê¹ì§€ ì•Šì•˜ê³ , ì‹œê°„ ê°€ëŠ” ì¤„ ëª°ëë‹¤.",
    "ë­˜ ë§í•˜ê³  ì‹¶ì€ì§€ë„ ëª¨ë¥´ê² ê³  ì „ì²´ì ìœ¼ë¡œ ë‚œí•´í•¨",
]

for s in sentences:
    label, prob = predict_sentiment(s)
    print(f"{s} â†’ {label} ({prob:.4f})")
```

---

**ğŸ“ˆ ì‹¤ì œ ì˜ˆì¸¡ ê²°ê³¼**
```
"ì§„ì§œ ë„ˆë¬´ ì¬ë°ŒëŠ” ì˜í™”ì˜€ë‹¤. ë˜ ë³´ê³  ì‹¶ë‹¤." â†’ ê¸ì • (0.9910)
"ìŠ¤í† ë¦¬ë„ ë³„ë¡œê³  ì—°ê¸°ë„ ì–´ìƒ‰í•´ì„œ ì‹œê°„ ì•„ê¹Œì› ë‹¤." â†’ ë¶€ì • (0.9988)
"ê·¸ëƒ¥ì €ëƒ¥ ë³¼ ë§Œí–ˆì§€ë§Œ ë‹¤ì‹œ ë³¼ ì •ë„ëŠ” ì•„ë‹ˆë‹¤." â†’ ë¶€ì • (0.9978)
"ëˆì´ ì „í˜€ ì•„ê¹ì§€ ì•Šì•˜ê³ , ì‹œê°„ ê°€ëŠ” ì¤„ ëª°ëë‹¤." â†’ ê¸ì • (0.9971)
"ì „ì²´ì ìœ¼ë¡œ ë‚œí•´í•˜ê³  ë­˜ ë§í•˜ê³  ì‹¶ì€ì§€ë„ ëª¨ë¥´ê² ë‹¤." â†’ ë¶€ì • (0.9988)
```

---

**ğŸ”® í–¥í›„ ê°œì„  ë°©ì•ˆ**

- ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”
- ì‚¬ì „í•™ìŠµ RoBERTa ê¸°ë°˜ ëª¨ë¸ ë¹„êµ
- ë¦¬ë·° ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì„ ìœ„í•œ Longformer ê³„ì—´ ëª¨ë¸ ì ìš©
- ê°ì„± ê°•ë„(ìŠ¬í””/ê¸°ì¨/ë¶„ë…¸ ë“±) ë‹¤ì¤‘ ë ˆì´ë¸” í™•ì¥

---

**ğŸ“œ ë¼ì´ì„ ìŠ¤**

- NSMC ë°ì´í„°ëŠ” Naver & e9tì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- KoBERTëŠ” SKTBrain ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
- KoELECTRAëŠ” monologgì˜ ê³µê°œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

**ğŸ”— Reference**

[NSMC (Naver Sentiment Movie Corpus)](https://github.com/e9t/nsmc)<br>
[KoBERT (SKT)](https://github.com/SKTBrain/KoBERT)<br>
[KoELECTRA (monologg)](https://github.com/monologg/KoELECTRA)
