# -*- coding: utf-8 -*-
"""KoBERT/KoELECTRA_NSMC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y3kWQ-bmrDw25wnwogvDEW0A0HG4rG-f
"""

!pip install -q transformers sentencepiece accelerate scikit-learn

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup,
)
from torch.optim import AdamW


from sklearn.metrics import accuracy_score, f1_score
from tqdm.auto import tqdm
import numpy as np
import random
import pandas as pd

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)

"""# NSMC"""

!git clone -q https://github.com/e9t/nsmc.git
!ls nsmc

train_df = pd.read_table("/content/nsmc/ratings_train.txt")
test_df  = pd.read_table("/content/nsmc/ratings_test.txt")

print(train_df.head())
print(train_df.shape, test_df.shape)

train_df = train_df.dropna()
test_df  = test_df.dropna()

# 문장 길이 0인 경우 제거
train_df = train_df[train_df["document"].str.len() > 0]
test_df  = test_df[test_df["document"].str.len() > 0]

train_df.shape, test_df.shape

class NSMCDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long),
        }

"""# KoBERT"""

MODEL_NAME = "monologg/kobert"

# KoBERT는 커스텀 토크나이저가 있어서 trust_remote_code=True 필요
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
)

# 하이퍼파라미터
MAX_LEN = 128
BATCH_SIZE = 32
EPOCHS = 3
LR = 5e-5

TRAIN_SIZE = 50000

small_train_df = train_df.sample(n=TRAIN_SIZE, random_state=42)

train_dataset = NSMCDataset(
    small_train_df["document"],
    small_train_df["label"],
    tokenizer,
    MAX_LEN,
)

valid_dataset = NSMCDataset(
    test_df["document"],
    test_df["label"],
    tokenizer,
    MAX_LEN,
)

len(train_dataset), len(valid_dataset)

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True if device.type == "cuda" else False,
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True if device.type == "cuda" else False,
)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2,          # NSMC: 부정(0) / 긍정(1)
)
model.to(device)

optimizer = AdamW(model.parameters(), lr=LR)

total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps,
)

def train_one_epoch(epoch, model, train_loader, optimizer, scheduler, device):
    model.train()
    total_loss = 0.0

    pbar = tqdm(train_loader, desc=f"[Epoch {epoch}] Train", leave=False)
    for batch in pbar:
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            labels=batch["labels"],
        )

        loss = outputs.loss

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / len(train_loader)


def evaluate(model, valid_loader, device):
    model.eval()
    total_loss = 0.0
    preds = []
    trues = []

    with torch.no_grad():
        pbar = tqdm(valid_loader, desc="[Eval]", leave=False)
        for batch in pbar:
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"],
                labels=batch["labels"],
            )

            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()
            preds.append(torch.argmax(logits, dim=-1).cpu())
            trues.append(batch["labels"].cpu())

    preds = torch.cat(preds).numpy()
    trues = torch.cat(trues).numpy()

    avg_loss = total_loss / len(valid_loader)
    acc = accuracy_score(trues, preds)
    f1 = f1_score(trues, preds)

    return avg_loss, acc, f1

best_acc = 0.0

for epoch in range(1, EPOCHS + 1):
    train_loss = train_one_epoch(epoch, model, train_loader, optimizer, scheduler, device)
    val_loss, val_acc, val_f1 = evaluate(model, valid_loader, device)

    print(f"===== Epoch {epoch}/{EPOCHS} =====")
    print(f"Train Loss: {train_loss:.4f}")
    print(f"Valid Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}")

    if val_acc > best_acc:
        best_acc = val_acc
        model.save_pretrained("kobert_nsmc_exp1")
        print(f"✅ Best model saved (Acc={best_acc:.4f})")

"""# KoELECTRA"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

MODEL_NAME = "monologg/koelectra-base-v3-discriminator"

tokenizer_elec = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    use_fast=True,          # fast tokenizer
    trust_remote_code=True,
)

MAX_LEN = 128
BATCH_SIZE = 32
EPOCHS = 3
LR = 3e-5

train_dataset_elec = NSMCDataset(
    train_df["document"],
    train_df["label"],
    tokenizer_elec,
    MAX_LEN,
)

valid_dataset_elec = NSMCDataset(
    test_df["document"],
    test_df["label"],
    tokenizer_elec,
    MAX_LEN,
)

train_loader_elec = DataLoader(
    train_dataset_elec,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=2,
    pin_memory=True if device.type == "cuda" else False,
)

valid_loader_elec = DataLoader(
    valid_dataset_elec,
    batch_size=BATCH_SIZE,
    shuffle=False,
    num_workers=2,
    pin_memory=True if device.type == "cuda" else False,
)

model_elec = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2,   # NSMC: 0=부정, 1=긍정
)
model_elec.to(device)

optimizer_elec = AdamW(model_elec.parameters(), lr=LR)

total_steps_elec = len(train_loader_elec) * EPOCHS
scheduler_elec = get_linear_schedule_with_warmup(
    optimizer_elec,
    num_warmup_steps=int(0.1 * total_steps_elec),
    num_training_steps=total_steps_elec,
)

best_acc_elec = 0.0

for epoch in range(1, EPOCHS + 1):
    train_loss = train_one_epoch(
        epoch,
        model_elec,
        train_loader_elec,
        optimizer_elec,
        scheduler_elec,
        device,
    )
    val_loss, val_acc, val_f1 = evaluate(
        model_elec,
        valid_loader_elec,
        device,
    )

    print(f"===== [KoELECTRA] Epoch {epoch}/{EPOCHS} =====")
    print(f"Train Loss: {train_loss:.4f}")
    print(f"Valid Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}")

    if val_acc > best_acc_elec:
        best_acc_elec = val_acc
        save_dir = "koelectra_nsmc_exp1"
        model_elec.save_pretrained(save_dir)
        print(f"✅ [KoELECTRA] Best model saved to '{save_dir}' (Acc={best_acc_elec:.4f})")

"""# TEST"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q transformers sentencepiece scikit-learn accelerate
!apt-get -y install git-lfs

!rm -rf Movie_review_sentiment_analysis
!git clone https://github.com/SolarHO/Movie_review_sentiment_analysis.git
# %cd Movie_review_sentiment_analysis

!git lfs install
!git lfs pull

!ls
!ls kobert_nsmc_exp2
!ls koelectra_nsmc_exp1

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score

device = torch.device("cpu")
print("Device:", device)

!rm -rf nsmc
!git clone -q https://github.com/e9t/nsmc.git

test_df = pd.read_table("nsmc/ratings_test.txt")
test_df = test_df.dropna()
test_df = test_df[test_df["document"].str.len() > 0]

print(test_df.shape)
test_df.head()

"""## KoBERT (kobert_nsmc_exp1)"""

kobert_model_dir = "kobert_nsmc_exp1"

kobert_model = AutoModelForSequenceClassification.from_pretrained(kobert_model_dir)
kobert_model.to(device)
kobert_model.eval()

kobert_tokenizer = AutoTokenizer.from_pretrained(
    "monologg/kobert",
    trust_remote_code=True,
)

MAX_LEN = 128
id2label = {0: "부정", 1: "긍정"}

"""## KoELECTRA (koelectra_nsmc_exp1)"""

koelectra_model_dir = "koelectra_nsmc_exp1"

koelectra_model = AutoModelForSequenceClassification.from_pretrained(koelectra_model_dir)
koelectra_model.to(device)
koelectra_model.eval()

koelectra_tokenizer = AutoTokenizer.from_pretrained(
    "monologg/koelectra-base-v3-discriminator",
    use_fast=True,
    trust_remote_code=True,
)

def predict_sentiment_kobert(text: str):
    inputs = kobert_tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN,
        return_tensors="pt",
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = kobert_model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=-1)

        pred_id = int(torch.argmax(probs, dim=-1).cpu().item())
        label = id2label[pred_id]
        prob = float(probs[0, pred_id].cpu().item())

    return label, prob


def predict_sentiment_koelectra(text: str):
    inputs = koelectra_tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN,
        return_tensors="pt",
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = koelectra_model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=-1)

        pred_id = int(torch.argmax(probs, dim=-1).cpu().item())
        label = id2label[pred_id]
        prob = float(probs[0, pred_id].cpu().item())

    return label, prob

sentences = [
    "진짜 너무 재밌는 영화였다. 또 보고 싶다.",
    "스토리도 별로고 연기도 어색해서 시간 아까웠다.",
    "그냥저냥 볼 만했지만 다시 볼 정도는 아니다.",
    "돈이 전혀 아깝지 않았고, 시간 가는 줄 몰랐다.",
    "뭘 말하고 싶은지도 모르겠고 전체적으로 난해함",
]

for s in sentences:
    k_label, k_p = predict_sentiment_kobert(s)
    e_label, e_p = predict_sentiment_koelectra(s)
    print(f"[문장] {s}")
    print(f"  KoBERT     → {k_label} (p={k_p:.4f})")
    print(f"  KoELECTRA  → {e_label} (p={e_p:.4f})")
    print()